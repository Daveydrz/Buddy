# ai/speech.py - Speech-to-text processing
import asyncio
import json
import numpy as np
import websockets
from config import FASTER_WHISPER_WS, DEBUG
import re
import os
from datetime import datetime

async def whisper_stt_async(audio):
    """Transcribe audio using Whisper WebSocket"""
    try:
        if audio.dtype != np.int16:
            if np.issubdtype(audio.dtype, np.floating):
                audio = (audio * 32767).clip(-32768, 32767).astype(np.int16)
            else:
                audio = audio.astype(np.int16)
        
        async with websockets.connect(FASTER_WHISPER_WS, ping_interval=None) as ws:
            await ws.send(audio.tobytes())
            await ws.send("end")
            
            try:
                message = await asyncio.wait_for(ws.recv(), timeout=15)
            except asyncio.TimeoutError:
                print("[Buddy V2] Whisper timeout")
                return ""
            
            try:
                data = json.loads(message)
                text = data.get("text", "").strip()
                if DEBUG:
                    print(f"[Buddy V2] üìù Whisper: '{text}'")
                return text
            except:
                text = message.decode("utf-8") if isinstance(message, bytes) else message
                if DEBUG:
                    print(f"[Buddy V2] üìù Whisper: '{text}'")
                return text.strip()
                
    except Exception as e:
        print(f"[Buddy V2] Whisper error: {e}")
        return ""

def extract_spoken_name(text: str, system_username: str) -> str:
    """Extract the user's actual spoken name using KoboldCPP intelligence"""
    
    try:
        # Try to import the ultra-intelligent name manager
        from voice.manager_names import UltraIntelligentNameManager
        
        # Create instance
        name_manager = UltraIntelligentNameManager()
        
        # Use the smart extraction
        if name_manager.is_ultra_intelligent_spontaneous_introduction(text):
            extracted_name = name_manager.extract_name_mega_intelligent(text)
            
            if extracted_name:
                # Don't use system usernames as spoken names
                if extracted_name.lower() != system_username.lower():
                    print(f"[Speech] ‚úÖ KoboldCPP extracted: {extracted_name}")
                    return extracted_name
                else:
                    print(f"[Speech] ‚ö†Ô∏è Extracted name matches system username: {extracted_name}")
                    return None
            else:
                print(f"[Speech] üõ°Ô∏è KoboldCPP blocked: '{text}'")
                return None
        else:
            print(f"[Speech] üõ°Ô∏è Not an introduction: '{text}'")
            return None
            
    except ImportError:
        print(f"[Speech] ‚ö†Ô∏è KoboldCPP name manager not available, using fallback")
        return extract_spoken_name_fallback(text, system_username)
    except Exception as e:
        print(f"[Speech] ‚ùå KoboldCPP error: {e}, using fallback")
        return extract_spoken_name_fallback(text, system_username)

def extract_spoken_name_fallback(text: str, system_username: str) -> str:
    """Fallback name extraction with better validation"""
    
    # More restrictive patterns that require explicit introduction
    name_patterns = [
        r"my\s+name\s+is\s+([a-zA-Z]+)",      # "My name is David"
        r"call\s+me\s+([a-zA-Z]+)",           # "Call me David"
        r"you\s+can\s+call\s+me\s+([a-zA-Z]+)", # "You can call me David"
        r"people\s+call\s+me\s+([a-zA-Z]+)",  # "People call me David"
        # Removed the problematic "i'm\s+([a-zA-Z]+)" pattern
    ]
    
    for pattern in name_patterns:
        match = re.search(pattern, text.lower())
        if match:
            spoken_name = match.group(1).capitalize()
            
            # Additional validation
            if is_valid_name_candidate(spoken_name, system_username):
                return spoken_name
    
    return None

def is_valid_name_candidate(name: str, system_username: str) -> bool:
    """Validate if extracted name is actually a name"""
    
    if not name or len(name) < 2:
        return False
    
    # Don't use system usernames as spoken names
    if name.lower() == system_username.lower():
        return False
    
    # Block common false positives
    false_positives = {
        'great', 'good', 'fine', 'okay', 'well', 'bad', 'tired', 'busy',
        'ready', 'sorry', 'happy', 'sad', 'angry', 'excited', 'confused',
        'here', 'there', 'home', 'work', 'back', 'away', 'done', 'going',
        'doing', 'working', 'thinking', 'checking', 'testing', 'trying'
    }
    
    if name.lower() in false_positives:
        print(f"[Speech] üõ°Ô∏è Blocked false positive: {name}")
        return False
    
    return True

def set_primary_identity(system_username: str, spoken_name: str):
    """Map system username to spoken name"""
    
    identity_file = f"memory/{system_username}/primary_identity.json"
    os.makedirs(f"memory/{system_username}", exist_ok=True)
    
    identity_data = {
        "system_username": system_username,
        "spoken_name": spoken_name,
        "display_name": spoken_name,
        "created_date": datetime.now().isoformat()
    }
    
    with open(identity_file, 'w') as f:
        json.dump(identity_data, f, indent=2)
    
    print(f"[Identity] Set primary identity: {system_username} ‚Üí {spoken_name}")

def get_display_name(system_username: str) -> str:
    """Get the user's preferred display name"""
    
    identity_file = f"memory/{system_username}/primary_identity.json"
    
    if os.path.exists(identity_file):
        with open(identity_file, 'r') as f:
            identity = json.load(f)
            return identity.get("spoken_name", system_username)
    
    return system_username

def identify_user(spoken_input: str, system_username: str) -> str:
    """Identify user and prevent duplicates with enhanced intelligence"""
    
    # Check if user introduced themselves using KoboldCPP
    spoken_name = extract_spoken_name(spoken_input, system_username)
    
    if spoken_name:
        # Set or update their primary identity
        set_primary_identity(system_username, spoken_name)
        print(f"[Identity] User identified as: {spoken_name} (system: {system_username})")
    else:
        # Debug: Show what was blocked
        print(f"[Identity] No name introduction detected in: '{spoken_input}'")
    
    # Always return the system username for memory storage
    # But display the spoken name in responses
    return system_username

def transcribe_audio(audio):
    """Synchronous wrapper for Whisper STT with proper event loop handling"""
    try:
        # ‚úÖ FIXED: Better event loop detection and handling
        current_loop = None
        loop_is_running = False
        
        try:
            # Try to get the current running loop (Python 3.7+)
            current_loop = asyncio.get_running_loop()
            loop_is_running = True
        except RuntimeError:
            # No running loop, which is fine
            pass
        
        if loop_is_running:
            # Running in an async context, use thread executor to avoid conflicts
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(lambda: asyncio.run(whisper_stt_async(audio)))
                return future.result(timeout=30)  # 30 second timeout for safety
        else:
            # No running loop, safe to use asyncio.run
            return asyncio.run(whisper_stt_async(audio))
            
    except Exception as e:
        print(f"[Speech] ‚ùå Async event loop error: {e}")
        print(f"[Speech] üîç Error details: {type(e).__name__}: {str(e)}")
        # Fallback to sync processing if async fails
        return _transcribe_audio_fallback(audio)

def _transcribe_audio_fallback(audio):
    """Fallback transcription method when async fails"""
    print("[Speech] ‚ö†Ô∏è Using fallback transcription method")
    
    # ‚úÖ ENHANCED: Try basic audio validation first
    try:
        import numpy as np
        
        if audio is None:
            print("[Speech] ‚ùå Fallback: Audio data is None")
            return "Audio transcription unavailable"
            
        if len(audio) == 0:
            print("[Speech] ‚ùå Fallback: Audio data is empty")
            return "Audio transcription unavailable"
            
        # Basic audio stats for debugging
        if hasattr(audio, '__len__'):
            duration_estimate = len(audio) / 16000  # Assuming 16kHz sample rate
            print(f"[Speech] üìä Fallback: Audio length ~{duration_estimate:.1f}s")
            
            if hasattr(audio, 'mean'):
                volume = np.abs(audio).mean()
                print(f"[Speech] üìä Fallback: Audio volume ~{volume:.1f}")
        
        # TODO: Could implement local transcription here
        # For now, return descriptive message
        print("[Speech] üí° Fallback: Consider implementing local transcription (e.g., OpenAI Whisper local)")
        
    except Exception as fallback_error:
        print(f"[Speech] ‚ö†Ô∏è Fallback audio analysis error: {fallback_error}")
    
    return "Audio transcription unavailable"